{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f10b838",
   "metadata": {},
   "source": [
    "## **File Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4221b2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arcpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m tfw_path = os.path.join(base_path, \u001b[33m\"\u001b[39m\u001b[33mcommunity.tfw\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m output_shp = os.path.join(base_path, \u001b[33m\"\u001b[39m\u001b[33mGTAA_shapefile\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m desc = \u001b[43marcpy\u001b[49m.Describe(dwg_path)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(desc.spatialReference.name)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(desc.spatialReference.factoryCode)\n",
      "\u001b[31mNameError\u001b[39m: name 'arcpy' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# base_path = r\"C:\\Users\\User\\Downloads\\Drawing sample 2\"\n",
    "\n",
    "# satellite_path = os.path.join(base_path, \"OP_communitymap..tif\")\n",
    "# cad_path = os.path.join(base_path, \"OP_ungeo.tif\")\n",
    "# output_gdb = os.path.join(base_path, \"output.gdb\")\n",
    "# dwg_path = os.path.join(base_path, \"10309 - Proposed Site Plan.dwg\")\n",
    "# tfw_path = os.path.join(base_path, \"OP_ungeo.tfw\")\n",
    "# output_shp = os.path.join(base_path, \"output_shapefile\")\n",
    "\n",
    "# base_path = r\"C:\\Users\\User\\Downloads\\Drawing sample 3\"\n",
    "\n",
    "# satellite_path = os.path.join(base_path, \"US_Virgin_Airpotrt_CAD_ungeoreferenced.tif\")\n",
    "# cad_path = os.path.join(base_path, \"US_Virgin_Airpotrt_Communit.tif\")\n",
    "# output_gdb = os.path.join(base_path, \"output.gdb\")\n",
    "# dwg_path = os.path.join(base_path, \"Ungeoreferenced_USVI.dwg\")\n",
    "# tfw_path = os.path.join(base_path, \"US_Virgin_Airpotrt_CAD_ungeoreferenced.tfw\")\n",
    "# output_shp = os.path.join(base_path, \"output_shapefile\")\n",
    "\n",
    "base_path = r\"C:\\Users\\User\\Downloads\\6414_Test\"\n",
    "\n",
    "satellite_path = os.path.join(base_path, \"GTAA_ungeoreferenced.tif\")\n",
    "cad_path = os.path.join(base_path, \"community.tif\")\n",
    "output_gdb = os.path.join(base_path, \"GTAA_output.gdb\")\n",
    "dwg_path = os.path.join(base_path, \"GTAA_cad.dwg\")\n",
    "tfw_path = os.path.join(base_path, \"community.tfw\")\n",
    "output_shp = os.path.join(base_path, \"GTAA_shapefile\")\n",
    "\n",
    "desc = arcpy.Describe(dwg_path)\n",
    "print(desc.spatialReference.name)\n",
    "print(desc.spatialReference.factoryCode)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eed827",
   "metadata": {},
   "source": [
    "## **Georeferencing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "def preprocess_image(image, target_size=(600, 600)):\n",
    "    \"\"\"Reduce background noise while preserving main structures for both satellite and CAD images.\"\"\"\n",
    "    img_resized = cv2.resize(image, target_size)\n",
    "    gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 15, 2)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    clean = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    return img_resized, gray, clean\n",
    "\n",
    "def add_border(image, border_size=100):\n",
    "    \"\"\"Add a larger border around the image to exclude areas from matching.\"\"\"\n",
    "    return cv2.copyMakeBorder(image, border_size, border_size, border_size, border_size, cv2.BORDER_CONSTANT, value=(255, 255, 255))\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    \"\"\"Rotate image by a given angle without cropping.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    return cv2.warpAffine(image, M, (w, h))\n",
    "\n",
    "def match_edge_maps(edges1, edges2, method=\"ORB\"):\n",
    "    \"\"\"Match keypoints using ORB or SIFT.\"\"\"\n",
    "    if method == \"ORB\":\n",
    "        detector = cv2.ORB_create(nfeatures=1000)\n",
    "    else:\n",
    "        detector = cv2.SIFT_create()\n",
    "    \n",
    "    kp1, des1 = detector.detectAndCompute(edges1, None)\n",
    "    kp2, des2 = detector.detectAndCompute(edges2, None)\n",
    "    \n",
    "    if des1 is None or des2 is None:\n",
    "        return None, [], []\n",
    "    \n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING if method == \"ORB\" else cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)[:100]\n",
    "    \n",
    "    return matches, kp1, kp2\n",
    "\n",
    "def calculate_angle(p1, p2):\n",
    "    \"\"\"Calculate the angle of the line between two points relative to the x-axis (in degrees),\n",
    "    ensuring the result is between 0 and 90 degrees.\"\"\"\n",
    "    dx = p2[0] - p1[0]\n",
    "    dy = p2[1] - p1[1]\n",
    "    angle = abs(math.atan2(dy, dx) * 180.0 / math.pi)  # Convert to degrees and ensure positive\n",
    "    angle = min(angle, 180 - angle)  # Ensure it's within 0-90 degrees\n",
    "    return angle\n",
    "\n",
    "def calculate_distance(p1, p2):\n",
    "    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
    "    return np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "\n",
    "def filter_extreme_slope_and_long_matches(matches, kp1, kp2, max_angle=45, max_distance_ratio=0.1):\n",
    "    \"\"\"Filter matches to remove those with extreme slopes or excessive distances.\"\"\"\n",
    "    filtered_matches = []\n",
    "    max_distance = 0  # To track the longest distance\n",
    "    \n",
    "    # First pass: calculate the maximum distance\n",
    "    for match in matches:\n",
    "        p1 = tuple(map(int, kp1[match.queryIdx].pt))\n",
    "        p2 = tuple(map(int, kp2[match.trainIdx].pt))\n",
    "        distance = calculate_distance(p1, p2)\n",
    "        max_distance = max(max_distance, distance)\n",
    "    \n",
    "    # Second pass: filter based on slope and distance\n",
    "    for match in matches:\n",
    "        p1 = tuple(map(int, kp1[match.queryIdx].pt))\n",
    "        p2 = tuple(map(int, kp2[match.trainIdx].pt))\n",
    "        \n",
    "        # Calculate the angle of the line formed by the match (p1, p2)\n",
    "        angle = calculate_angle(p1, p2)\n",
    "        \n",
    "        # Calculate the Euclidean distance between the points\n",
    "        distance = calculate_distance(p1, p2)\n",
    "        \n",
    "        # Keep match only if:\n",
    "        # 1. The angle is less than the max_angle threshold (not too steep)\n",
    "        # 2. The distance is within a certain ratio of the max distance (not too long)\n",
    "        if abs(angle) <= max_angle and distance <= max_distance * max_distance_ratio:\n",
    "            filtered_matches.append(match)\n",
    "    \n",
    "    return filtered_matches\n",
    "\n",
    "def filter_quadrant_matches(matches, kp1, kp2, img1_shape, img2_shape):\n",
    "    \"\"\"Filter matches to ensure that points are within the same quadrant in both images.\"\"\"\n",
    "    h1, w1 = img1_shape[:2]  # Ignore the channel dimension\n",
    "    h2, w2 = img2_shape[:2]  # Ignore the channel dimension\n",
    "    \n",
    "    valid_matches = []\n",
    "    for match in matches:\n",
    "        p1 = kp1[match.queryIdx].pt\n",
    "        p2 = kp2[match.trainIdx].pt\n",
    "        \n",
    "        # Determine quadrants for both images\n",
    "        quadrant1 = (p1[0] < w1 // 2, p1[1] < h1 // 2)\n",
    "        quadrant2 = (p2[0] < w2 // 2, p2[1] < h2 // 2)\n",
    "        \n",
    "        # Keep match only if they are in the same quadrant\n",
    "        if quadrant1 == quadrant2:\n",
    "            valid_matches.append(match)\n",
    "    \n",
    "    return valid_matches\n",
    "\n",
    "def visualize_matches(img1, kp1, img2, kp2, matches, title):\n",
    "    \"\"\"Draw and visualize matched keypoints side by side.\"\"\"\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR) if len(img1.shape) == 2 else img1\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR) if len(img2.shape) == 2 else img2\n",
    "    \n",
    "    new_h, new_w = max(h1, h2), w1 + w2\n",
    "    canvas = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "    canvas[:h1, :w1] = img1\n",
    "    canvas[:h2, w1:w1+w2] = img2\n",
    "    \n",
    "    for match in matches:\n",
    "        kp1_pt = kp1[match.queryIdx].pt\n",
    "        kp2_pt = (kp2[match.trainIdx].pt[0] + w1, kp2[match.trainIdx].pt[1])\n",
    "        color = tuple(random.randint(0, 255) for _ in range(3))\n",
    "        cv2.line(canvas, tuple(map(int, kp1_pt)), tuple(map(int, kp2_pt)), color, 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# satellite_path = r\"C:\\Users\\User\\Downloads\\TIFF GTAA Georeferenced (1)\\TIFF GTAA Georeferenced\\community.tif\"\n",
    "# cad_path = r\"C:\\Users\\User\\Downloads\\6414_Test\\GTAA_ungeoreferenced.tif\"\n",
    "img1 = cv2.imread(satellite_path)\n",
    "img2 = cv2.imread(cad_path)\n",
    "\n",
    "# Add a larger border around the satellite image\n",
    "border_size = 100\n",
    "img1_with_border = add_border(img1, border_size)\n",
    "img2_with_border = add_border(img2, border_size)\n",
    "\n",
    "img1_processed, gray1, edges1 = preprocess_image(img1_with_border)\n",
    "img2_processed, gray2, edges2 = preprocess_image(img2_with_border)\n",
    "\n",
    "best_matches, best_kp1, best_kp2, best_angle = [], [], [], 0\n",
    "for angle in range(0, 360, 10):\n",
    "    edges2_rot = rotate_image(edges2, angle)\n",
    "    matches, kp1, kp2 = match_edge_maps(edges1, edges2_rot, method=\"ORB\")\n",
    "    if matches and len(matches) > len(best_matches):\n",
    "        best_matches, best_kp1, best_kp2, best_angle = matches, kp1, kp2, angle\n",
    "\n",
    "# Filter matches to ensure they are in the same quadrant\n",
    "best_matches = filter_quadrant_matches(best_matches, best_kp1, best_kp2, img1.shape, img2.shape)\n",
    "\n",
    "# Filter for extreme slopes and long distances\n",
    "best_matches = filter_extreme_slope_and_long_matches(best_matches, best_kp1, best_kp2, max_angle=45, max_distance_ratio=0.1)\n",
    "\n",
    "#visualize_matches(img1_processed, best_kp1, img2_processed, best_kp2, best_matches, f\"ORB Best Match at {best_angle}°\")\n",
    "\n",
    "matches_sift, kp1_sift, kp2_sift = match_edge_maps(edges1, edges2, method=\"SIFT\")\n",
    "if matches_sift:\n",
    "    #matches_sift = filter_quadrant_matches(matches_sift, kp1_sift, kp2_sift, img1.shape, img2.shape)\n",
    "    matches_sift = filter_extreme_slope_and_long_matches(matches_sift, kp1_sift, kp2_sift, max_angle=45, max_distance_ratio=0.1)\n",
    "    visualize_matches(img1_processed, kp1_sift, img2_processed, kp2_sift, matches_sift, \"SIFT Matches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f1731",
   "metadata": {},
   "source": [
    "## **CAD to GIS Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21531d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import arcpy\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "def extract_matched_points(matches, kp1, kp2):\n",
    "    \"\"\"Extract (x, y) coordinates from matched keypoints.\"\"\"\n",
    "    cad_points = np.array([kp1[m.queryIdx].pt for m in matches])  # CAD image\n",
    "    map_points = np.array([kp2[m.trainIdx].pt for m in matches])  # Map image\n",
    "    return cad_points, map_points\n",
    "\n",
    "def transform_map_point_to_geospatial(px, py, tfw_params):\n",
    "    \"\"\"Convert pixel coordinates (px, py) in the map image to geospatial coordinates using TFW.\"\"\"\n",
    "    A, D, B, E, C, F = tfw_params\n",
    "    geo_x = A * px + B * py + C\n",
    "    geo_y = D * px + E * py + F\n",
    "    return geo_x, geo_y\n",
    "\n",
    "def compute_dwg_bounding_box(dwg_path):\n",
    "    \"\"\"Extract the DWG bounding box (minX, maxX, minY, maxY) using ArcPy.\"\"\"\n",
    "    desc = arcpy.Describe(dwg_path)\n",
    "    extent = desc.extent\n",
    "    return extent.XMin, extent.XMax, extent.YMin, extent.YMax\n",
    "\n",
    "def transform_cad_to_dwg(px, py, cad_size, dwg_bbox):\n",
    "    \"\"\"Convert pixel coordinates in the CAD image to DWG coordinate system.\"\"\"\n",
    "    minX, maxX, minY, maxY = dwg_bbox\n",
    "    img_width, img_height = cad_size\n",
    "    scale_x = (maxX - minX) / img_width\n",
    "    scale_y = (maxY - minY) / img_height\n",
    "    dwg_x = minX + px * scale_x\n",
    "    dwg_y = maxY - py * scale_y  # Flip Y-axis\n",
    "    return dwg_x, dwg_y\n",
    "\n",
    "def read_tfw(tfw_path):\n",
    "    with open(tfw_path, \"r\") as f:\n",
    "        values = [float(line.strip()) for line in f.readlines()]\n",
    "    return values  # [A, D, B, E, C, F]\n",
    "\n",
    "def georeference_cad(dwg_path, control_points, output_gdb, output_shp):\n",
    "    \"\"\"Use control points to georeference CAD and convert to GIS format.\"\"\"\n",
    "    desc = arcpy.Describe(dwg_path)\n",
    "    spatial_ref = arcpy.SpatialReference(26917)\n",
    "\n",
    "    fc_name = f\"control_points_{uuid.uuid4().hex[:8]}\"\n",
    "    control_points_fc = os.path.join(output_gdb, fc_name)\n",
    "    print(control_points_fc)\n",
    "\n",
    "    if arcpy.Exists(control_points_fc):\n",
    "        arcpy.Delete_management(control_points_fc)\n",
    "\n",
    "    print(\"Contents of GDB:\")\n",
    "    arcpy.env.workspace = output_gdb\n",
    "    print(arcpy.ListFeatureClasses())\n",
    "\n",
    "    arcpy.env.workspace = output_gdb\n",
    "    result = arcpy.CreateFeatureclass_management(\n",
    "        output_gdb,\n",
    "        fc_name,\n",
    "        \"POINT\",\n",
    "        spatial_reference=spatial_ref\n",
    "    )\n",
    "\n",
    "    print(\"Feature class created at:\", result)\n",
    "    print(\"Exists immediately after creation?\", arcpy.Exists(control_points_fc))\n",
    "    time.sleep(5)\n",
    "\n",
    "    print(\"Exists?\", arcpy.Exists(control_points_fc))\n",
    "    \n",
    "    with arcpy.da.InsertCursor(control_points_fc, ['SHAPE@XY']) as cursor:\n",
    "        for dwg_pt, geo_pt in control_points:\n",
    "            print(\"Inserting point:\", dwg_pt)\n",
    "            cursor.insertRow([(dwg_pt[0], dwg_pt[1])])\n",
    "    \n",
    "    # Convert CAD to GIS\n",
    "    arcpy.CADToGeodatabase_conversion(\n",
    "        dwg_path,\n",
    "        output_gdb,\n",
    "        \"cad_features\",\n",
    "        \"1\",  # reference scale as string\n",
    "        spatial_ref\n",
    "    )\n",
    "    \n",
    "    # Export EACH feature class inside cad_features dataset to individual shapefiles\n",
    "    cad_feature_dataset = os.path.join(output_gdb, \"cad_features\")\n",
    "    feature_classes = arcpy.ListFeatureClasses(feature_dataset=\"cad_features\")\n",
    "\n",
    "    if not os.path.exists(output_shp):\n",
    "        os.makedirs(output_shp)\n",
    "\n",
    "    for fc in feature_classes:\n",
    "        full_fc_path = os.path.join(cad_feature_dataset, fc)\n",
    "        print(f\"Exporting: {fc} -> {output_shp}\")\n",
    "        arcpy.conversion.FeatureClassToShapefile([full_fc_path], output_shp)\n",
    "\n",
    "    print(\"All CAD features exported to shapefiles.\")\n",
    "    \n",
    "def main(dwg_path, map_image, cad_image, tfw_path, matches, kp1, kp2, output_gdb, output_shp):\n",
    "    \"\"\"Main function to process matches and georeference CAD to GIS.\"\"\"\n",
    "    # Step 1: Extract matched pixel coordinates\n",
    "    cad_points, map_points = extract_matched_points(matches, kp1, kp2)\n",
    "    \n",
    "    # Step 2: Compute the original size of images\n",
    "    cad_size = cad_image.shape[1], cad_image.shape[0]  # (width, height)\n",
    "    map_size = map_image.shape[1], map_image.shape[0]\n",
    "    \n",
    "    # Step 3: Convert map points to geospatial coordinates\n",
    "    tfw_params = read_tfw(tfw_path)\n",
    "    geospatial_points_map = [transform_map_point_to_geospatial(x, y, tfw_params) for x, y in map_points]\n",
    "    \n",
    "    # Step 4: Compute DWG bounding box\n",
    "    dwg_bbox = compute_dwg_bounding_box(dwg_path)\n",
    "    \n",
    "    # Step 5: Convert CAD points to DWG coordinates\n",
    "    dwg_points = [transform_cad_to_dwg(x, y, cad_size, dwg_bbox) for x, y in cad_points]\n",
    "     \n",
    "    # Step 6: Create control points and georeference the CAD file\n",
    "    control_points = list(zip(dwg_points, geospatial_points_map))\n",
    "    georeference_cad(dwg_path, control_points, output_gdb, output_shp)\n",
    "    \n",
    "    print(\"Georeferencing complete. CAD converted to GIS format and shapefile exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not arcpy.Exists(output_gdb):\n",
    "    arcpy.CreateFileGDB_management(out_folder_path=os.path.dirname(output_gdb),\n",
    "                                    out_name=os.path.basename(output_gdb))\n",
    "    \n",
    "desc = arcpy.Describe(dwg_path)\n",
    "print(desc.spatialReference.name)\n",
    "print(desc.spatialReference.factoryCode)\n",
    "    \n",
    "main(dwg_path = dwg_path,\n",
    "     map_image=cv2.imread(satellite_path),\n",
    "     cad_image=cv2.imread(cad_path),\n",
    "     tfw_path= tfw_path,  # Extract these from the TFW file\n",
    "     matches=matches_sift, \n",
    "     kp1=kp1_sift, \n",
    "     kp2=kp2_sift,\n",
    "     output_gdb=output_gdb,\n",
    "     output_shp=output_shp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66528699",
   "metadata": {},
   "source": [
    "## **Quality Control (Error Detection)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86441af4",
   "metadata": {},
   "source": [
    "### **Feature Extraction** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f975738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# NUMBER OF DATA POINTS\n",
    "NUM_DATA_POINTS = 10000\n",
    "\n",
    "print('Reading Shapefile...')\n",
    "\n",
    "# Load the shapefile\n",
    "shapefile_path = os.path.join(base_path, 'GTAA_errors', 'GTAA_errors.shp')\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "print('Shapefile Loaded!\\n')\n",
    "\n",
    "# exploding\n",
    "gdf = gdf.explode(index_parts=True).reset_index()\n",
    "\n",
    "class DataProcessing:\n",
    "    completedCount = 0\n",
    "\n",
    "    def remove_duplicates(self, gdf):\n",
    "        gdf_temp = gdf.iloc[:, 1:].copy()\n",
    "        gdf_temp = gdf_temp.drop(columns=['Error'])\n",
    "        dup_mask = gdf_temp.duplicated(keep='first')\n",
    "\n",
    "        gdf_res = gdf[~dup_mask]\n",
    "        gdf_res.loc[:, 'Error'] = gdf_res['Error'].apply(lambda x: 0 if x == 2 else x)\n",
    "        return gdf_res\n",
    "\n",
    "    def compute_linestring_metrics(self, linestring):\n",
    "        \"\"\"\n",
    "        Computes multiple metrics for a LINESTRING Z:\n",
    "        - is_closed: Whether the linestring forms a closed path.\n",
    "        - length_3d: The total 3D length of the linestring.\n",
    "        - line_curvature: The ratio of actual path length to straight-line distance.\n",
    "        - num_vertices: The number of vertices in the linestring.\n",
    "        - vertex_density: The number of vertices per unit length.\n",
    "        - total_angle_change: The sum of absolute angle changes between segments.\n",
    "        - is_point: Whether the linestring consists of a single repeated point.\n",
    "        - num_connections: Number of other lines that touch this line, but don't go through (i.e. a 3-way intersection, not a 4-way intersection)\n",
    "        - num_intersections: Number of other lines that pass through this line\n",
    "        - bounding_box_width: The width of the bounding box of the linestring.\n",
    "        - bounding_box_height: The height of the bounding box of the linestring.\n",
    "        - relative_angle_change: Ratio of total_angle_change to length of the line. (Relative Angle Change to Line Length)\n",
    "        - proximity_to_neighbors: The average minimum distance to neighboring lines.\n",
    "        - similarity_score: Similarity score with close neighbor lines based on selected metrics (length, curvature, vertices) and normalized based on neighbor group\n",
    "        - num_neighbors: Number of neighboring lines within a radius equal to 1% of the total map size.\n",
    "        \"\"\"\n",
    "        if not isinstance(linestring, LineString) or len(linestring.coords) < 2:\n",
    "            return False, 0.0, 1.0, 0, 0.0, 0.0, True, 0, 0 ,0.0, 0.0, 0.0, 0.0, 0.0, 0 # Defaults for invalid linestrings\n",
    "\n",
    "        coords = np.array(linestring.coords)\n",
    "        first_point = coords[0]\n",
    "        last_point = coords[-1]\n",
    "        is_closed = np.array_equal(first_point, last_point)\n",
    "\n",
    "        diffs = np.diff(coords, axis=0)\n",
    "        segment_lengths = np.linalg.norm(diffs, axis=1)\n",
    "        sum_length = np.sum(segment_lengths)\n",
    "\n",
    "        straight_line_distance = np.linalg.norm(last_point - first_point)\n",
    "        line_curvature = sum_length / straight_line_distance if straight_line_distance > 0 else 1.0\n",
    "\n",
    "        num_vertices = len(coords)\n",
    "\n",
    "        # Vertices per unit length\n",
    "        vertex_density = num_vertices / sum_length if sum_length > 0 else 0.0\n",
    "\n",
    "        # Compute total angle change\n",
    "        if len(linestring.coords) == 2:\n",
    "            total_angle_change = 0.0\n",
    "        else:\n",
    "            valid_segments = segment_lengths > 0\n",
    "            unit_vectors = diffs[valid_segments] / segment_lengths[:, None][valid_segments]\n",
    "            dot_products = np.einsum(\"ij,ij->i\", unit_vectors[:-1], unit_vectors[1:])\n",
    "            angles = np.arccos(np.clip(dot_products, -1.0, 1.0))\n",
    "            # Ensure we sum all angle changes without taking absolute values\n",
    "            total_angle_change = np.sum(angles)\n",
    "\n",
    "        # Check if all coordinates are the same (i.e., single repeated point)\n",
    "        is_point = np.all(coords == coords[0])\n",
    "\n",
    "        # Compute number of connections (lines that share at least one point)\n",
    "        connected_lines_idx = list(spatial_index.intersection(linestring.bounds))\n",
    "        connected_lines = gdf.iloc[connected_lines_idx]\n",
    "        # Subtract 1 to exclude the line itself\n",
    "        num_connections = sum(connected_lines.geometry.touches(linestring)) \n",
    "\n",
    "        # Compute number of lines that intersect with this line\n",
    "        intersecting_lines_idx = list(\n",
    "            spatial_index.intersection(linestring.bounds))\n",
    "        intersecting_lines = gdf.iloc[intersecting_lines_idx]\n",
    "        # Subtract 1 to exclude the line itself\n",
    "        num_intersections = sum(intersecting_lines.geometry.intersects(linestring)) - 1\n",
    "\n",
    "        # Compute bounding box width and height\n",
    "        minx, miny, maxx, maxy = linestring.bounds\n",
    "        bounding_box_width = maxx - minx\n",
    "        bounding_box_height = maxy - miny\n",
    "\n",
    "        # Get the bounding box of the current line\n",
    "        buffered_line = linestring.buffer(search_radius)\n",
    "        possible_neighbors = list(spatial_index.intersection(buffered_line.bounds))\n",
    "\n",
    "        num_neighbors = len(possible_neighbors) - 1\n",
    "\n",
    "        # Compute proximity to neighbors (average minimum distance)\n",
    "        min_distances = []\n",
    "        for neighbor_idx in possible_neighbors:\n",
    "            neighbor = gdf.iloc[neighbor_idx].geometry\n",
    "            if neighbor != linestring:  # Avoid self-comparison\n",
    "                distance = linestring.distance(neighbor)\n",
    "                min_distances.append(distance)\n",
    "\n",
    "        # If there are neighbors, compute the average distance\n",
    "        proximity_to_neighbors = search_radius * 2\n",
    "        # Compute weighted proximity: Closer neighbors have higher weight\n",
    "        if min_distances:\n",
    "            weights = [1 / d if d > 0 else 1 for d in min_distances]\n",
    "            proximity_to_neighbors = np.average(min_distances, weights=weights)\n",
    "        else:\n",
    "            proximity_to_neighbors = search_radius * 2  # Default if no neighbors\n",
    "\n",
    "        # Compute similarity score with neighbors\n",
    "        similarity_score = 0.0\n",
    "        neighbor_metrics = []\n",
    "        for neighbor_idx in possible_neighbors:\n",
    "            neighbor = gdf.iloc[neighbor_idx].geometry\n",
    "            if neighbor != linestring:  # Avoid self-comparison\n",
    "                neighbor_coords = np.array(neighbor.coords)\n",
    "                neighbor_diffs = np.diff(neighbor_coords, axis=0)\n",
    "                neighbor_segment_lengths = np.linalg.norm(neighbor_diffs, axis=1)\n",
    "                neighbor_sum_length = np.sum(neighbor_segment_lengths)\n",
    "\n",
    "                neighbor_first_point = coords[0]\n",
    "                neighbor_last_point = coords[-1]\n",
    "                neighbor_straight_line_distance = np.linalg.norm(\n",
    "                    neighbor_last_point - neighbor_first_point)\n",
    "                neighbor_line_curvature = neighbor_sum_length / \\\n",
    "                    neighbor_straight_line_distance if neighbor_straight_line_distance > 0 else 1.0\n",
    "                \n",
    "                neighbor_num_vertices = len(neighbor_coords)\n",
    "\n",
    "                neighbor_metrics.append(\n",
    "                    [neighbor_sum_length, neighbor_line_curvature, neighbor_num_vertices])\n",
    "\n",
    "        # Extract and normalize metrics for the neighbor group\n",
    "        lengths = np.array([metrics[0] for metrics in neighbor_metrics])\n",
    "        curvatures = np.array([metrics[1] for metrics in neighbor_metrics])\n",
    "        vertices = np.array([metrics[2] for metrics in neighbor_metrics])\n",
    "\n",
    "        # Normalize the current line's metrics against the neighbor group\n",
    "        length_mean, length_std = np.mean(lengths), np.std(lengths)\n",
    "        curvature_mean, curvature_std = np.mean(curvatures), np.std(curvatures)\n",
    "        vertices_mean, vertices_std = np.mean(vertices), np.std(vertices)\n",
    "\n",
    "        normalized_length = (sum_length - length_mean) / \\\n",
    "            length_std if length_std > 0 else 0.0\n",
    "        normalized_curvature = (line_curvature - curvature_mean) / \\\n",
    "            curvature_std if curvature_std > 0 else 0.0\n",
    "        normalized_vertices = (num_vertices - vertices_mean) / \\\n",
    "            vertices_std if vertices_std > 0 else 0.0\n",
    "\n",
    "        # Calculate similarity score using normalized metrics\n",
    "        if (num_neighbors > 0):\n",
    "            similarity_score = 1 / (1 + abs(normalized_length) + abs(normalized_curvature) +\n",
    "                                abs(normalized_vertices))\n",
    "\n",
    "        relative_angle_change = total_angle_change / sum_length if sum_length != 0 else 0\n",
    "\n",
    "        self.completedCount += 1\n",
    "        if self.completedCount % 1000 == 0:\n",
    "            print(f'Finished {self.completedCount} rows')\n",
    "\n",
    "        return is_closed, sum_length, line_curvature, num_vertices, vertex_density, total_angle_change, is_point, num_connections, num_intersections, bounding_box_width, bounding_box_height, relative_angle_change, proximity_to_neighbors, similarity_score, num_neighbors\n",
    "\n",
    "gdf = gdf.reset_index()\n",
    "gdf['id'] = gdf['index']\n",
    "\n",
    "processing = DataProcessing()\n",
    "# Shuffle the GeoDataFrame and take the top 10,000 rows\n",
    "# Shuffling the GeoDataFrame\n",
    "gdf = processing.remove_duplicates(gdf)\n",
    "\n",
    "gdf1 = gdf[gdf['Error'] == 1]\n",
    "gdf2 = gdf[gdf['Error'] != 1]\n",
    "gdf2 = gdf2.head(NUM_DATA_POINTS)  # Take the top 10,000 rows\n",
    "gdf = gpd.GeoDataFrame(pd.concat([gdf1, gdf2], ignore_index=True))\n",
    "\n",
    "gdf = gdf.sample(frac=1, random_state=42)\n",
    "gdf = gdf.reset_index(drop=True)\n",
    "spatial_index = gdf.sindex\n",
    "\n",
    "print('Duplicates Removed + Random Sampling + Set Spacial Index\\n')\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "\n",
    "# Compute the width and height of the bounding box\n",
    "bbox_width = maxx - minx\n",
    "bbox_height = maxy - miny\n",
    "search_radius = 0.01 * max(bbox_width, bbox_height)\n",
    "\n",
    "print('Computing Features...')\n",
    "# Apply the function and ensure the output shape matches the expected columns\n",
    "metrics = gdf.geometry.apply(processing.compute_linestring_metrics)\n",
    "print('Features Computed!\\n')\n",
    "\n",
    "# Convert the result to a DataFrame and check the lengths\n",
    "metrics_df = pd.DataFrame(metrics.tolist(), columns=[\n",
    "                          \"is_closed\", \"sum_length\", \"line_curvature\", \"num_vertices\", \"vertex_density\", \"total_angle_change\", \"is_point\", \"num_connections\", \"num_intersections\", \"bounding_box_width\", \"bounding_box_height\", \"relative_angle_change\", \"proximity_to_neighbors\", \"similarity_score\", \"num_neighbors\"])\n",
    "\n",
    "# Assign the calculated metrics to the original GeoDataFrame\n",
    "gdf = gdf.join(metrics_df)\n",
    "\n",
    "# Compute metrics for each line\n",
    "columns = [\"id\", \"is_closed\", \"sum_length\", \"line_curvature\", \"num_vertices\", \"vertex_density\",\n",
    "           \"total_angle_change\", \"is_point\", \"num_connections\", \"num_intersections\", \"bounding_box_width\", \"bounding_box_height\", \"relative_angle_change\", \"proximity_to_neighbors\", \"similarity_score\", \"num_neighbors\", \"Error\"]\n",
    "gdf_features = gdf[columns]\n",
    "gdf_features = gdf_features.reset_index()\n",
    "print('Final Training Set Created with Labels')\n",
    "\n",
    "output_path = os.path.join(base_path, 'GTAA_Errors_with_metrics.csv')\n",
    "gdf_features.to_csv(output_path, index=False)\n",
    "print('Training Set Stored')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec209dfe",
   "metadata": {},
   "source": [
    "### **Error Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bdc81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bfc8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "   index      id  is_closed  sum_length  line_curvature  num_vertices  \\\n",
      "0      0  156838      False    5.799891        2.707294             6   \n",
      "1      1    6898       True   14.995678        1.000000            17   \n",
      "2      2    6941       True    7.000716        1.000000             5   \n",
      "3      3     509      False    0.901641        1.000000             2   \n",
      "4      4    9176      False    0.680246        1.000000             2   \n",
      "\n",
      "   vertex_density  total_angle_change  is_point  num_connections  \\\n",
      "0        1.034502            5.269789     False                0   \n",
      "1        1.133660           23.564956     False                0   \n",
      "2        0.714213            4.712721     False                0   \n",
      "3        2.218177            0.000000     False                0   \n",
      "4        2.940115            0.000000     False                0   \n",
      "\n",
      "   num_intersections  bounding_box_width  bounding_box_height  \\\n",
      "0                  1              2.0073               2.1411   \n",
      "1                  2              2.9789               2.9592   \n",
      "2                  0              2.5874               2.3029   \n",
      "3                  0              0.6290               0.6460   \n",
      "4                  0              0.1836               0.6550   \n",
      "\n",
      "   relative_angle_change  proximity_to_neighbors  similarity_score  \\\n",
      "0               0.908601               10.992623          0.323633   \n",
      "1               1.571450                4.710499          0.487796   \n",
      "2               0.673177                2.689995          0.618077   \n",
      "3               0.000000                5.571404          0.362947   \n",
      "4               0.000000                6.536263          0.475676   \n",
      "\n",
      "   num_neighbors  Error  \n",
      "0            300      1  \n",
      "1             23      0  \n",
      "2             61      0  \n",
      "3             88      0  \n",
      "4            110      0  \n",
      "\n",
      "Training set:\n",
      "Total data points: 8483\n",
      "Error\n",
      "0    94.188377\n",
      "1     5.811623\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set:\n",
      "Total data points: 2121\n",
      "Error\n",
      "0    94.76662\n",
      "1     5.23338\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_67240\\2137943297.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace({\"TRUE\": 1, \"FALSE\": 0, True: 1, False: 0}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load dataset\n",
    "feature_path = os.path.join(base_path, \"GTAA_Errors_with_metrics.csv\")\n",
    "df = pd.read_csv(feature_path) \n",
    "\n",
    "# Print dataset head for debugging\n",
    "print(\"Dataset preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Convert is_point columns from TRUE/FALSE to 1/0\n",
    "df.replace({\"TRUE\": 1, \"FALSE\": 0, True: 1, False: 0}, inplace=True)\n",
    "\n",
    "# Identify feature and target columns\n",
    "ignore_cols = [\"id\", \"index\"]\n",
    "target_col = \"Error\"\n",
    "features = df.drop(columns=ignore_cols + [target_col])\n",
    "target = df[target_col]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nTraining set:\")\n",
    "print(f\"Total data points: {len(y_train)}\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "print(f\"Total data points: {len(y_test)}\")\n",
    "print(y_test.value_counts(normalize=True) * 100)\n",
    "\n",
    "# Standardize features for SVM\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec4dd89",
   "metadata": {},
   "source": [
    "### **Error Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afdca20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 493, number of negative: 7990\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2645\n",
      "[LightGBM] [Info] Number of data points in the train set: 8483, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.058116 -> initscore=-2.785437\n",
      "[LightGBM] [Info] Start training from score -2.785437\n",
      "\n",
      "LightGBM - Train set:\n",
      "Accuracy: 1.0000\n",
      "F1-score: 1.0000\n",
      "False Positives: 0\n",
      "False Negatives: 0\n",
      "Percentage of Errors Correctly Identified: 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7990\n",
      "           1       1.00      1.00      1.00       493\n",
      "\n",
      "    accuracy                           1.00      8483\n",
      "   macro avg       1.00      1.00      1.00      8483\n",
      "weighted avg       1.00      1.00      1.00      8483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM - Test set:\n",
      "Accuracy: 0.9995\n",
      "F1-score: 0.9955\n",
      "False Positives: 0\n",
      "False Negatives: 1\n",
      "Percentage of Errors Correctly Identified: 99.10%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2010\n",
      "           1       1.00      0.99      1.00       111\n",
      "\n",
      "    accuracy                           1.00      2121\n",
      "   macro avg       1.00      1.00      1.00      2121\n",
      "weighted avg       1.00      1.00      1.00      2121\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 394, number of negative: 6392\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2644\n",
      "[LightGBM] [Info] Number of data points in the train set: 6786, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.058061 -> initscore=-2.786452\n",
      "[LightGBM] [Info] Start training from score -2.786452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 394, number of negative: 6392\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2637\n",
      "[LightGBM] [Info] Number of data points in the train set: 6786, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.058061 -> initscore=-2.786452\n",
      "[LightGBM] [Info] Start training from score -2.786452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 394, number of negative: 6392\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2640\n",
      "[LightGBM] [Info] Number of data points in the train set: 6786, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.058061 -> initscore=-2.786452\n",
      "[LightGBM] [Info] Start training from score -2.786452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 395, number of negative: 6392\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2642\n",
      "[LightGBM] [Info] Number of data points in the train set: 6787, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.058199 -> initscore=-2.783917\n",
      "[LightGBM] [Info] Start training from score -2.783917\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 395, number of negative: 6392\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2641\n",
      "[LightGBM] [Info] Number of data points in the train set: 6787, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.058199 -> initscore=-2.783917\n",
      "[LightGBM] [Info] Start training from score -2.783917\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "LightGBM - Cross-validation Accuracy (5-fold): 0.9989 ± 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "name = \"LightGBM\"\n",
    "    \n",
    "# For models requiring scaling (SVM, XGBoost, GradientBoosting, LightGBM)\n",
    "if name in [\"SVM (Polynomial Kernel)\", \"XGBoost\", \"Gradient Boosting\", \"LightGBM\"]:\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "else:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "for dataset, y_true, y_pred in [(\"Train\", y_train, y_train_pred), (\"Test\", y_test, y_test_pred)]:\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    percent_errors_identified = (tp / (tp + fn)) * 100 if (tp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{name} - {dataset} set:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"Percentage of Errors Correctly Identified: {percent_errors_identified:.2f}%\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_scaled if name in [\"SVM (Polynomial Kernel)\", \"XGBoost\", \"Gradient Boosting\", \"LightGBM\"] else X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"\\n{name} - Cross-validation Accuracy (5-fold): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "728a1fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefile Loaded!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd \n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "features_scaled = feature_scaler.fit_transform(features)\n",
    "\n",
    "error_predictions = model.predict(features_scaled)\n",
    "df[\"prediction\"] = error_predictions\n",
    "\n",
    "# Load the shapefile\n",
    "shapefile_path = os.path.join(base_path, 'GTAA_errors', 'GTAA_errors.shp')\n",
    "features_gdf = gpd.read_file(shapefile_path)\n",
    "print('Shapefile Loaded!\\n')\n",
    "\n",
    "# exploding\n",
    "features_gdf = features_gdf.explode(index_parts=True).reset_index()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08a88b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_67240\\4017676373.py:14: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  gdf_filtered.to_file(predicted_shpfile, driver=\"ESRI Shapefile\")\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'Error Prediction' to 'Error Pred'\n",
      "  ogr_write(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Layer</th>\n",
       "      <th>LyrFrzn</th>\n",
       "      <th>EntColor</th>\n",
       "      <th>LyrColor</th>\n",
       "      <th>Linetype</th>\n",
       "      <th>LineWt</th>\n",
       "      <th>EntLineWt</th>\n",
       "      <th>LyrLineWt</th>\n",
       "      <th>BlkLineWt</th>\n",
       "      <th>RefName</th>\n",
       "      <th>Error</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>geometry</th>\n",
       "      <th>Error Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>635</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>4D5</td>\n",
       "      <td>SP-E-RNW-PV</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>XREF1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.962188</td>\n",
       "      <td>LINESTRING Z (610773.181 4837863.142 0, 610774...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>2288</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>4D5</td>\n",
       "      <td>SP-E-RNW-PV</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>XREF1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.325395</td>\n",
       "      <td>LINESTRING Z (610323.359 4837152.198 0, 610324...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>3009</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>4D5</td>\n",
       "      <td>SP-E-RNW-PV</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>XREF1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376205</td>\n",
       "      <td>LINESTRING Z (610213.129 4837143.046 0, 610213...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>3010</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>4D5</td>\n",
       "      <td>SP-E-RNW-PV</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>XREF1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.838672</td>\n",
       "      <td>LINESTRING Z (610213.255 4837142.906 0, 610212...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9398</th>\n",
       "      <td>9398</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>4D5</td>\n",
       "      <td>SP-E-RNW-PV</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>XREF1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.298599</td>\n",
       "      <td>LINESTRING Z (609408.777 4837168.723 0, 609407...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157716</th>\n",
       "      <td>157716</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>43559A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Buildings</td>\n",
       "      <td>1</td>\n",
       "      <td>1.403000</td>\n",
       "      <td>LINESTRING Z (611343.528 4837643.454 0, 611344...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157722</th>\n",
       "      <td>157722</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>43559A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Buildings</td>\n",
       "      <td>1</td>\n",
       "      <td>3.889863</td>\n",
       "      <td>LINESTRING Z (611381.206 4837589.397 0, 611383...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157723</th>\n",
       "      <td>157723</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>43559A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Buildings</td>\n",
       "      <td>1</td>\n",
       "      <td>1.049936</td>\n",
       "      <td>LINESTRING Z (611381.637 4837589.307 0, 611381...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157724</th>\n",
       "      <td>157724</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>43559A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Buildings</td>\n",
       "      <td>1</td>\n",
       "      <td>5.801748</td>\n",
       "      <td>LINESTRING Z (611381.418 4837587.632 0, 611383...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157725</th>\n",
       "      <td>157725</td>\n",
       "      <td>0</td>\n",
       "      <td>Insert</td>\n",
       "      <td>43559A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Buildings</td>\n",
       "      <td>1</td>\n",
       "      <td>1.402992</td>\n",
       "      <td>LINESTRING Z (611383.399 4837586.079 0, 611383...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>488 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        level_0  level_1  Entity  Handle        Layer  LyrFrzn  EntColor  \\\n",
       "635         635        0  Insert     4D5  SP-E-RNW-PV        0        -1   \n",
       "2288       2288        0  Insert     4D5  SP-E-RNW-PV        0        -1   \n",
       "3009       3009        0  Insert     4D5  SP-E-RNW-PV        0        -1   \n",
       "3010       3010        0  Insert     4D5  SP-E-RNW-PV        0        -1   \n",
       "9398       9398        0  Insert     4D5  SP-E-RNW-PV        0        -1   \n",
       "...         ...      ...     ...     ...          ...      ...       ...   \n",
       "157716   157716        0  Insert  43559A            0        0        -1   \n",
       "157722   157722        0  Insert  43559A            0        0        -1   \n",
       "157723   157723        0  Insert  43559A            0        0        -1   \n",
       "157724   157724        0  Insert  43559A            0        0        -1   \n",
       "157725   157725        0  Insert  43559A            0        0        -1   \n",
       "\n",
       "        LyrColor    Linetype  LineWt  EntLineWt  LyrLineWt  BlkLineWt  \\\n",
       "635            7  Continuous      15         -1         15         -1   \n",
       "2288           7  Continuous      15         -1         15         -1   \n",
       "3009           7  Continuous      15         -1         15         -1   \n",
       "3010           7  Continuous      15         -1         15         -1   \n",
       "9398           7  Continuous      15         -1         15         -1   \n",
       "...          ...         ...     ...        ...        ...        ...   \n",
       "157716         7  Continuous      25         -1         -3         -1   \n",
       "157722         7  Continuous      25         -1         -3         -1   \n",
       "157723         7  Continuous      25         -1         -3         -1   \n",
       "157724         7  Continuous      25         -1         -3         -1   \n",
       "157725         7  Continuous      25         -1         -3         -1   \n",
       "\n",
       "          RefName  Error  Shape_Leng  \\\n",
       "635         XREF1      0    0.962188   \n",
       "2288        XREF1      0    1.325395   \n",
       "3009        XREF1      0    0.376205   \n",
       "3010        XREF1      0    0.838672   \n",
       "9398        XREF1      0    1.298599   \n",
       "...           ...    ...         ...   \n",
       "157716  Buildings      1    1.403000   \n",
       "157722  Buildings      1    3.889863   \n",
       "157723  Buildings      1    1.049936   \n",
       "157724  Buildings      1    5.801748   \n",
       "157725  Buildings      1    1.402992   \n",
       "\n",
       "                                                 geometry  Error Prediction  \n",
       "635     LINESTRING Z (610773.181 4837863.142 0, 610774...                 1  \n",
       "2288    LINESTRING Z (610323.359 4837152.198 0, 610324...                 1  \n",
       "3009    LINESTRING Z (610213.129 4837143.046 0, 610213...                 1  \n",
       "3010    LINESTRING Z (610213.255 4837142.906 0, 610212...                 1  \n",
       "9398    LINESTRING Z (609408.777 4837168.723 0, 609407...                 1  \n",
       "...                                                   ...               ...  \n",
       "157716  LINESTRING Z (611343.528 4837643.454 0, 611344...                 1  \n",
       "157722  LINESTRING Z (611381.206 4837589.397 0, 611383...                 1  \n",
       "157723  LINESTRING Z (611381.637 4837589.307 0, 611381...                 1  \n",
       "157724  LINESTRING Z (611381.418 4837587.632 0, 611383...                 1  \n",
       "157725  LINESTRING Z (611383.399 4837586.079 0, 611383...                 1  \n",
       "\n",
       "[488 rows x 18 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set df index to 'id' to enable fast matching\n",
    "df_indexed = df.set_index('id')\n",
    "\n",
    "# Optionally: keep only relevant gdf rows\n",
    "gdf_filtered = gdf[gdf.index.isin(df_indexed.index)].copy()\n",
    "\n",
    "# Add new column to gdf by mapping the 'prediction' from df\n",
    "gdf_filtered['Error Prediction'] = df_indexed.loc[gdf_filtered.index, 'prediction'].values\n",
    "gdf_filtered.loc[gdf_filtered['Error'] == 2, 'Error'] = 0\n",
    "\n",
    "predicted_shpfolder= os.path.join(base_path, 'GTAA_prediction')\n",
    "os.makedirs(predicted_shpfolder, exist_ok=True)\n",
    "predicted_shpfile = os.path.join(predicted_shpfolder, 'GTAA_prediction.shp')\n",
    "gdf_filtered.to_file(predicted_shpfile, driver=\"ESRI Shapefile\")\n",
    "\n",
    "\n",
    "gdf_filtered[gdf_filtered['Error Prediction'] == 1].head(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
